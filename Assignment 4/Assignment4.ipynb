{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab6580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Upgrade gymnasium to latest version\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gymnasium\", \"--upgrade\"])\n",
    "!pip install swig\n",
    "!pip install \"gymnasium[box2d]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6870b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "WANDB_API_KEY = \"2c421b79022678408a8dec66cb629dc5d1708474\" \n",
    "wandb.login(key=WANDB_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceef87e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Imports, Device, Seeding\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import torch.optim as optim\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Device\n",
    "# ------------------------------------------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Seeding utilities\n",
    "# ------------------------------------------------------------\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "class FrameSkip(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Repeat the same action for n frames and accumulate reward.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, skip=4):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        info = {}\n",
    "\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        return obs, total_reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "def make_env(env_name: str, seed: int, render_mode=None):\n",
    "    env = gym.make(env_name, render_mode=render_mode, continuous=True)\n",
    "    env = FrameSkip(env, skip=4)\n",
    "\n",
    "    env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ac209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3: Replay Buffer & Utilities\n",
    "# ============================================================\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Standard Replay Buffer for Off-Policy Algorithms (SAC, TD3).\"\"\"\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # state, next_state: np arrays (vector or image)\n",
    "        self.buffer.append(Transition(state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        state_batch = torch.as_tensor(\n",
    "            np.array([t.state for t in transitions]), dtype=torch.float32, device=DEVICE\n",
    "        )\n",
    "        action_batch = torch.as_tensor(\n",
    "            np.array([t.action for t in transitions]), dtype=torch.float32, device=DEVICE\n",
    "        )\n",
    "        reward_batch = torch.as_tensor(\n",
    "            np.array([t.reward for t in transitions]), dtype=torch.float32, device=DEVICE\n",
    "        ).unsqueeze(-1)\n",
    "        next_state_batch = torch.as_tensor(\n",
    "            np.array([t.next_state for t in transitions]), dtype=torch.float32, device=DEVICE\n",
    "        )\n",
    "        done_batch = torch.as_tensor(\n",
    "            np.array([t.done for t in transitions]), dtype=torch.float32, device=DEVICE\n",
    "        ).unsqueeze(-1)\n",
    "        \n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def get_action_bounds(env):\n",
    "    \"\"\"Returns low and high action bounds of a continuous action space.\"\"\"\n",
    "    return env.action_space.low, env.action_space.high\n",
    "\n",
    "def scale_action(action_unscaled, min_action, max_action):\n",
    "    \"\"\"Scale action from [-1, 1] to [min_action, max_action].\"\"\"\n",
    "    return min_action + (max_action - min_action) * (action_unscaled + 1.0) / 2.0\n",
    "\n",
    "def unscale_action(action_scaled, min_action, max_action):\n",
    "    \"\"\"Inverse of scale_action: map [min_action, max_action] -> [-1, 1].\"\"\"\n",
    "    return (action_scaled - min_action) / (max_action - min_action) * 2.0 - 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e44ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 4: Shared CNN/MLP Feature Extractor\n",
    "# ============================================================\n",
    "\n",
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -20\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Handles both vector observations (e.g., LunarLander)\n",
    "    and image observations (e.g., CarRacing).\n",
    "    - For vectors: simple MLP to 256 features.\n",
    "    - For images: CNN -> FC to 512 features.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_shape):\n",
    "        super().__init__()\n",
    "        self.obs_shape = obs_shape\n",
    "        \n",
    "        if len(obs_shape) == 1:\n",
    "            # Vector observation\n",
    "            self.is_image = False\n",
    "            input_dim = obs_shape[0]\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.Linear(input_dim, 256),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "            self.output_dim = 256\n",
    "        else:\n",
    "            # Image observation (H, W, C)\n",
    "            self.is_image = True\n",
    "            h, w, c = obs_shape\n",
    "            conv_layers = [\n",
    "                nn.Conv2d(c,   32, kernel_size=8, stride=4), nn.ReLU(),\n",
    "                nn.Conv2d(32,  64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "                nn.Conv2d(64,  64, kernel_size=3, stride=1), nn.ReLU(),\n",
    "            ]\n",
    "            self.cnn = nn.Sequential(*conv_layers)\n",
    "            \n",
    "            # Compute the size after convs\n",
    "            with torch.no_grad():\n",
    "                dummy = torch.zeros(1, c, h, w)\n",
    "                n_flatten = self.cnn(dummy).view(1, -1).shape[1]\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(n_flatten, 512),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            self.output_dim = 512\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: tensor of shape\n",
    "           - (B, obs_dim) for vector\n",
    "           - (B, H, W, C) or (H, W, C) for image\n",
    "        \"\"\"\n",
    "        if not self.is_image:\n",
    "            if x.dim() == 1:\n",
    "                x = x.unsqueeze(0)\n",
    "            return self.mlp(x)\n",
    "\n",
    "        # Image path\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(0)  # (1, H, W, C)\n",
    "        if x.shape[1] != self.obs_shape[2]:\n",
    "            # currently (B, H, W, C); convert to (B, C, H, W)\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        x = x / 255.0\n",
    "        x = self.cnn(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88b8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Policy & Value Networks (PPO/SAC/TD3)\n",
    "# ============================================================\n",
    "\n",
    "class PPOActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic network for PPO.\n",
    "    Uses FeatureExtractor internally to handle both LL-v3 and CarRacing-v3.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_shape, action_dim):\n",
    "        super().__init__()\n",
    "        self.feature = FeatureExtractor(obs_shape)\n",
    "        feat_dim = self.feature.output_dim\n",
    "        \n",
    "        self.actor_mean = nn.Linear(feat_dim, action_dim)\n",
    "        self.actor_log_std = nn.Parameter(torch.zeros(1, action_dim))\n",
    "        self.critic = nn.Linear(feat_dim, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        features = self.feature(state)\n",
    "        mean = torch.tanh(self.actor_mean(features))  # [-1, 1]\n",
    "        log_std = self.actor_log_std.expand_as(mean)\n",
    "        std = torch.exp(log_std)\n",
    "        value = self.critic(features)\n",
    "        return mean, std, value\n",
    "\n",
    "\n",
    "class StochasticActor(nn.Module):\n",
    "    \"\"\"\n",
    "    Stochastic actor for SAC.\n",
    "    Outputs mu and log_std.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_shape, action_dim):\n",
    "        super().__init__()\n",
    "        self.feature = FeatureExtractor(obs_shape)\n",
    "        feat_dim = self.feature.output_dim\n",
    "        \n",
    "        self.mu_layer = nn.Linear(feat_dim, action_dim)\n",
    "        self.log_std_layer = nn.Linear(feat_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        feat = self.feature(state)\n",
    "        mu = self.mu_layer(feat)\n",
    "        log_std = self.log_std_layer(feat)\n",
    "        log_std = torch.tanh(log_std)\n",
    "        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)\n",
    "        return mu, log_std\n",
    "\n",
    "\n",
    "class DeterministicActor(nn.Module):\n",
    "    \"\"\"\n",
    "    Deterministic actor for TD3.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_shape, action_dim):\n",
    "        super().__init__()\n",
    "        self.feature = FeatureExtractor(obs_shape)\n",
    "        feat_dim = self.feature.output_dim\n",
    "        \n",
    "        self.mu_layer = nn.Linear(feat_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        feat = self.feature(state)\n",
    "        mu = self.mu_layer(feat)\n",
    "        return torch.tanh(mu)\n",
    "\n",
    "\n",
    "class QFunction(nn.Module):\n",
    "    \"\"\"\n",
    "    Critic network Q(s,a) for SAC/TD3.\n",
    "    Uses a FeatureExtractor (shared) for state.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_shape, action_dim):\n",
    "        super().__init__()\n",
    "        self.feature = FeatureExtractor(obs_shape)\n",
    "        feat_dim = self.feature.output_dim\n",
    "        self.q_net = nn.Sequential(\n",
    "            nn.Linear(feat_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        feat = self.feature(state)\n",
    "        if action.dim() == 1:\n",
    "            action = action.unsqueeze(0)\n",
    "        x = torch.cat([feat, action], dim=-1)\n",
    "        return self.q_net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e822b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 6: PPO Agent\n",
    "# ============================================================\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, obs_shape, action_dim, hyperparams, action_bounds):\n",
    "        self.GAMMA = hyperparams['gamma']\n",
    "        self.LR = hyperparams['learning_rate']\n",
    "        self.CLIP_EPSILON = hyperparams['clip_epsilon']\n",
    "        self.PPO_EPOCHS = hyperparams['ppo_epochs']\n",
    "        self.MINIBATCH_SIZE = hyperparams['minibatch_size']\n",
    "        self.ENTROPY_COEFF = hyperparams['entropy_coeff']\n",
    "        self.ACTION_MIN, self.ACTION_MAX = action_bounds\n",
    "        self.obs_shape = obs_shape\n",
    "\n",
    "        self.model = PPOActorCritic(obs_shape, action_dim).to(DEVICE)\n",
    "        self.old_model = PPOActorCritic(obs_shape, action_dim).to(DEVICE)\n",
    "        self.old_model.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.LR)\n",
    "\n",
    "        self.buffer = {'s': [], 'a': [], 'logp': [], 'v': [], 'r': [], 'm': []}\n",
    "\n",
    "    def _state_to_tensor(self, state):\n",
    "        t = torch.as_tensor(state, dtype=torch.float32, device=DEVICE)\n",
    "        return t\n",
    "\n",
    "    def select_action(self, state, deterministic=False):\n",
    "        state_tensor = self._state_to_tensor(state)\n",
    "        mean, std, value = self.model(state_tensor)\n",
    "\n",
    "        if deterministic:\n",
    "            action_unscaled = mean\n",
    "        else:\n",
    "            dist = Normal(mean, std)\n",
    "            action_unscaled = dist.rsample()\n",
    "\n",
    "        dist = Normal(mean, std)\n",
    "        log_prob = dist.log_prob(action_unscaled).sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # scale action to env bounds\n",
    "        action_unscaled_np = action_unscaled.detach().cpu().numpy().flatten()\n",
    "        action_scaled = scale_action(action_unscaled_np, self.ACTION_MIN, self.ACTION_MAX)\n",
    "\n",
    "        # store transition data for PPO\n",
    "        self.buffer['s'].append(state)  # store raw state (np)\n",
    "        self.buffer['a'].append(action_unscaled_np)\n",
    "        self.buffer['logp'].append(log_prob.detach().cpu().numpy().flatten())\n",
    "        self.buffer['v'].append(value.detach().cpu().numpy().flatten())\n",
    "\n",
    "        return action_scaled\n",
    "\n",
    "    def store_reward(self, reward, done):\n",
    "        self.buffer['r'].append(reward)\n",
    "        self.buffer['m'].append(1.0 - float(done))\n",
    "\n",
    "    def clear_buffer(self):\n",
    "        self.buffer = {'s': [], 'a': [], 'logp': [], 'v': [], 'r': [], 'm': []}\n",
    "\n",
    "    def _compute_gae_and_returns(self, last_value):\n",
    "        rewards = self.buffer['r']\n",
    "        masks = self.buffer['m']\n",
    "        values = np.concatenate(self.buffer['v'])\n",
    "\n",
    "        # discounted returns\n",
    "        returns = []\n",
    "        R = last_value\n",
    "        for reward, mask in zip(reversed(rewards), reversed(masks)):\n",
    "            R = reward + self.GAMMA * R * mask\n",
    "            returns.append(R)\n",
    "        returns = np.array(list(reversed(returns))).flatten()\n",
    "\n",
    "        advantage = returns - values\n",
    "        advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
    "        return returns, advantage\n",
    "\n",
    "    def learn(self, last_state):\n",
    "        if not self.buffer['r']:\n",
    "            return None\n",
    "\n",
    "        if last_state is None:\n",
    "            last_value = 0.0\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = self._state_to_tensor(last_state)\n",
    "                _, _, last_value_tensor = self.model(state_tensor)\n",
    "                last_value = last_value_tensor.item()\n",
    "\n",
    "        returns, advantages = self._compute_gae_and_returns(last_value)\n",
    "\n",
    "        state_batch = torch.as_tensor(\n",
    "            np.array(self.buffer['s']), dtype=torch.float32, device=DEVICE\n",
    "        )\n",
    "        action_batch = torch.as_tensor(\n",
    "            np.array(self.buffer['a']), dtype=torch.float32, device=DEVICE\n",
    "        )\n",
    "        old_logp_batch = torch.as_tensor(\n",
    "            np.array(self.buffer['logp']), dtype=torch.float32, device=DEVICE\n",
    "        ).unsqueeze(-1)\n",
    "        return_batch = torch.as_tensor(\n",
    "            returns, dtype=torch.float32, device=DEVICE\n",
    "        ).unsqueeze(-1)\n",
    "        advantage_batch = torch.as_tensor(\n",
    "            advantages, dtype=torch.float32, device=DEVICE\n",
    "        ).unsqueeze(-1)\n",
    "\n",
    "        self.old_model.load_state_dict(self.model.state_dict())\n",
    "        total_loss = 0.0\n",
    "\n",
    "        data_size = len(state_batch)\n",
    "        indices = np.arange(data_size)\n",
    "\n",
    "        for _ in range(self.PPO_EPOCHS):\n",
    "            np.random.shuffle(indices)\n",
    "            for start in range(0, data_size, self.MINIBATCH_SIZE):\n",
    "                end = start + self.MINIBATCH_SIZE\n",
    "                batch_idx = indices[start:end]\n",
    "\n",
    "                s = state_batch[batch_idx]\n",
    "                a = action_batch[batch_idx]\n",
    "                old_logp = old_logp_batch[batch_idx]\n",
    "                ret = return_batch[batch_idx]\n",
    "                adv = advantage_batch[batch_idx]\n",
    "\n",
    "                mean, std, value = self.model(s)\n",
    "                dist = Normal(mean, std)\n",
    "                logp = dist.log_prob(a).sum(dim=-1, keepdim=True)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                ratio = torch.exp(logp - old_logp)\n",
    "                surr1 = ratio * adv\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.CLIP_EPSILON, 1.0 + self.CLIP_EPSILON) * adv\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                critic_loss = F.mse_loss(value, ret)\n",
    "\n",
    "                loss = actor_loss + critic_loss - self.ENTROPY_COEFF * entropy\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        self.clear_buffer()\n",
    "        return total_loss / max(self.PPO_EPOCHS, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f1114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 7: SAC Agent\n",
    "# ============================================================\n",
    "\n",
    "class SACAgent:\n",
    "    def __init__(self, obs_shape, action_dim, hyperparams, action_bounds):\n",
    "        self.GAMMA = hyperparams['gamma']\n",
    "        self.LR_ACTOR = hyperparams['learning_rate']\n",
    "        self.LR_CRITIC = hyperparams.get('lr_critic', 3e-4)\n",
    "        self.LR_ALPHA = hyperparams.get('lr_alpha', 1e-4)\n",
    "        self.TAU = hyperparams['tau']\n",
    "        self.BATCH_SIZE = hyperparams['batch_size']\n",
    "        self.MEMORY_CAPACITY = hyperparams['memory_size']\n",
    "        self.ACTION_MIN, self.ACTION_MAX = action_bounds\n",
    "        self.obs_shape = obs_shape\n",
    "\n",
    "        # Entropy regularization\n",
    "        alpha_start = hyperparams.get('alpha_start', 0.1)\n",
    "        self.log_alpha = torch.tensor(\n",
    "            np.log(alpha_start),\n",
    "            dtype=torch.float32,\n",
    "            requires_grad=True,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "        self.alpha = self.log_alpha.exp()\n",
    "        self.target_entropy = -torch.tensor(action_dim, dtype=torch.float32, device=DEVICE)\n",
    "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=self.LR_ALPHA)\n",
    "\n",
    "        # Networks\n",
    "        self.actor = StochasticActor(obs_shape, action_dim).to(DEVICE)\n",
    "        self.q1 = QFunction(obs_shape, action_dim).to(DEVICE)\n",
    "        self.q2 = QFunction(obs_shape, action_dim).to(DEVICE)\n",
    "        self.q1_target = QFunction(obs_shape, action_dim).to(DEVICE)\n",
    "        self.q2_target = QFunction(obs_shape, action_dim).to(DEVICE)\n",
    "\n",
    "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.LR_ACTOR)\n",
    "        self.q_params = list(self.q1.parameters()) + list(self.q2.parameters())\n",
    "        self.q_optimizer = optim.Adam(self.q_params, lr=self.LR_CRITIC)\n",
    "\n",
    "        self.memory = ReplayBuffer(self.MEMORY_CAPACITY)\n",
    "\n",
    "    def _state_to_tensor(self, state):\n",
    "        return torch.as_tensor(state, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    def _get_action_and_logp(self, state_tensor):\n",
    "        mu, log_std = self.actor(state_tensor)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu, std)\n",
    "        z = dist.rsample()\n",
    "        action = torch.tanh(z)\n",
    "        log_prob = dist.log_prob(z).sum(dim=-1, keepdim=True)\n",
    "        log_prob -= torch.sum(torch.log(torch.clamp(1 - action.pow(2), 1e-6, 1.0)), dim=-1, keepdim=True)\n",
    "        return action, log_prob\n",
    "\n",
    "    def select_action(self, state, deterministic=False):\n",
    "        state_tensor = self._state_to_tensor(state)\n",
    "        with torch.no_grad():\n",
    "            mu, log_std = self.actor(state_tensor)\n",
    "            if deterministic:\n",
    "                action_unscaled = torch.tanh(mu)\n",
    "            else:\n",
    "                action_unscaled, _ = self._get_action_and_logp(state_tensor)\n",
    "\n",
    "            action_unscaled_np = action_unscaled.cpu().numpy().flatten()\n",
    "            action_scaled = scale_action(action_unscaled_np, self.ACTION_MIN, self.ACTION_MAX)\n",
    "            return action_scaled\n",
    "\n",
    "    def store_transition(self, s, a, r, s_prime, done):\n",
    "        # store *unscaled* action in buffer (-1,1)\n",
    "        unscaled_a = unscale_action(a, self.ACTION_MIN, self.ACTION_MAX)\n",
    "        self.memory.push(s, unscaled_a, r, s_prime, done)\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return None\n",
    "\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.memory.sample(self.BATCH_SIZE)\n",
    "\n",
    "        # Critic update\n",
    "        with torch.no_grad():\n",
    "            next_action, next_log_prob = self._get_action_and_logp(next_state_batch)\n",
    "            q1_target_val = self.q1_target(next_state_batch, next_action)\n",
    "            q2_target_val = self.q2_target(next_state_batch, next_action)\n",
    "            min_q_target = torch.min(q1_target_val, q2_target_val)\n",
    "            target_v = min_q_target - self.alpha * next_log_prob\n",
    "            q_target = reward_batch + self.GAMMA * (1 - done_batch) * target_v\n",
    "\n",
    "        q1_val = self.q1(state_batch, action_batch)\n",
    "        q2_val = self.q2(state_batch, action_batch)\n",
    "\n",
    "        q1_loss = F.mse_loss(q1_val, q_target)\n",
    "        q2_loss = F.mse_loss(q2_val, q_target)\n",
    "        critic_loss = q1_loss + q2_loss\n",
    "\n",
    "        self.q_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.q_optimizer.step()\n",
    "\n",
    "        # Actor update\n",
    "        for p in self.q_params:\n",
    "            p.requires_grad = False\n",
    "\n",
    "        new_action, log_prob = self._get_action_and_logp(state_batch)\n",
    "        q1_new = self.q1(state_batch, new_action)\n",
    "        q2_new = self.q2(state_batch, new_action)\n",
    "        min_q_new = torch.min(q1_new, q2_new)\n",
    "\n",
    "        actor_loss = (self.alpha * log_prob - min_q_new).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        for p in self.q_params:\n",
    "            p.requires_grad = True\n",
    "\n",
    "        # Alpha update\n",
    "        alpha_loss = -(self.log_alpha * (log_prob + self.target_entropy).detach()).mean()\n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "        self.alpha = self.log_alpha.exp().item()\n",
    "\n",
    "        # Soft update of target networks\n",
    "        for target_param, param in zip(self.q1_target.parameters(), self.q1.parameters()):\n",
    "            target_param.data.copy_(self.TAU * param.data + (1 - self.TAU) * target_param.data)\n",
    "        for target_param, param in zip(self.q2_target.parameters(), self.q2.parameters()):\n",
    "            target_param.data.copy_(self.TAU * param.data + (1 - self.TAU) * target_param.data)\n",
    "\n",
    "        return critic_loss.item() + actor_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d25c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 8: TD3 Agent\n",
    "# ============================================================\n",
    "\n",
    "class TD3Agent:\n",
    "    def __init__(self, obs_shape, action_dim, hyperparams, action_bounds):\n",
    "        self.GAMMA = hyperparams['gamma']\n",
    "        self.LR_ACTOR = hyperparams['learning_rate']\n",
    "        self.LR_CRITIC = hyperparams.get('lr_critic', 1e-3)\n",
    "        self.TAU = hyperparams['tau']\n",
    "        self.BATCH_SIZE = hyperparams['batch_size']\n",
    "        self.MEMORY_CAPACITY = hyperparams['memory_size']\n",
    "        self.POLICY_DELAY = hyperparams.get('policy_delay', 2)\n",
    "        self.POLICY_NOISE = hyperparams.get('policy_noise', 0.2)\n",
    "        self.NOISE_CLIP = hyperparams.get('noise_clip', 0.5)\n",
    "\n",
    "        self.ACTION_MIN, self.ACTION_MAX = action_bounds\n",
    "        self.obs_shape = obs_shape\n",
    "        self.critic_updates = 0\n",
    "\n",
    "        self.actor = DeterministicActor(obs_shape, action_dim).to(DEVICE)\n",
    "        self.actor_target = DeterministicActor(obs_shape, action_dim).to(DEVICE)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        self.q1 = QFunction(obs_shape, action_dim).to(DEVICE)\n",
    "        self.q2 = QFunction(obs_shape, action_dim).to(DEVICE)\n",
    "        self.q1_target = QFunction(obs_shape, action_dim).to(DEVICE)\n",
    "        self.q2_target = QFunction(obs_shape, action_dim).to(DEVICE)\n",
    "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.LR_ACTOR)\n",
    "        self.q_params = list(self.q1.parameters()) + list(self.q2.parameters())\n",
    "        self.q_optimizer = optim.Adam(self.q_params, lr=self.LR_CRITIC)\n",
    "\n",
    "        self.memory = ReplayBuffer(self.MEMORY_CAPACITY)\n",
    "\n",
    "    def _state_to_tensor(self, state):\n",
    "        return torch.as_tensor(state, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    def select_action(self, state, deterministic=False):\n",
    "        state_tensor = self._state_to_tensor(state)\n",
    "        with torch.no_grad():\n",
    "            action_unscaled = self.actor(state_tensor)\n",
    "            if not deterministic:\n",
    "                noise = torch.randn_like(action_unscaled) * 0.1\n",
    "                action_unscaled = torch.clamp(action_unscaled + noise, -1.0, 1.0)\n",
    "            action_unscaled_np = action_unscaled.cpu().numpy().flatten()\n",
    "            action_scaled = scale_action(action_unscaled_np, self.ACTION_MIN, self.ACTION_MAX)\n",
    "            return action_scaled\n",
    "\n",
    "    def store_transition(self, s, a, r, s_prime, done):\n",
    "        unscaled_a = unscale_action(a, self.ACTION_MIN, self.ACTION_MAX)\n",
    "        self.memory.push(s, unscaled_a, r, s_prime, done)\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return None\n",
    "\n",
    "        self.critic_updates += 1\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.memory.sample(self.BATCH_SIZE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            noise = (torch.randn_like(action_batch) * self.POLICY_NOISE).clamp(-self.NOISE_CLIP, self.NOISE_CLIP)\n",
    "            next_action_unscaled = self.actor_target(next_state_batch)\n",
    "            next_action = torch.clamp(next_action_unscaled + noise, -1.0, 1.0)\n",
    "\n",
    "            q1_target_val = self.q1_target(next_state_batch, next_action)\n",
    "            q2_target_val = self.q2_target(next_state_batch, next_action)\n",
    "            min_q_target = torch.min(q1_target_val, q2_target_val)\n",
    "\n",
    "            q_target = reward_batch + self.GAMMA * (1 - done_batch) * min_q_target\n",
    "\n",
    "        q1_val = self.q1(state_batch, action_batch)\n",
    "        q2_val = self.q2(state_batch, action_batch)\n",
    "\n",
    "        q1_loss = F.mse_loss(q1_val, q_target)\n",
    "        q2_loss = F.mse_loss(q2_val, q_target)\n",
    "        critic_loss = q1_loss + q2_loss\n",
    "\n",
    "        self.q_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.q_optimizer.step()\n",
    "\n",
    "        actor_loss = None\n",
    "        if self.critic_updates % self.POLICY_DELAY == 0:\n",
    "            q_actor = self.q1(state_batch, self.actor(state_batch))\n",
    "            actor_loss = -q_actor.mean()\n",
    "\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Soft updates\n",
    "            for target_param, param in zip(self.q1_target.parameters(), self.q1.parameters()):\n",
    "                target_param.data.copy_(self.TAU * param.data + (1 - self.TAU) * target_param.data)\n",
    "            for target_param, param in zip(self.q2_target.parameters(), self.q2.parameters()):\n",
    "                target_param.data.copy_(self.TAU * param.data + (1 - self.TAU) * target_param.data)\n",
    "            for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "                target_param.data.copy_(self.TAU * param.data + (1 - self.TAU) * target_param.data)\n",
    "\n",
    "        return critic_loss.item() + (actor_loss.item() if actor_loss is not None else 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a97abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 9: Training & Testing Loops\n",
    "# ============================================================\n",
    "\n",
    "def train_agent(env_name, agent, algo_name, hyperparams, num_timesteps, seed=0, log_wandb=True):\n",
    "    env = make_env(env_name, seed=seed, render_mode=None)\n",
    "    total_steps = 0\n",
    "    episodes = 0\n",
    "\n",
    "    ppo_steps_per_update = hyperparams.get('trajectory_size', num_timesteps + 1)\n",
    "\n",
    "    while total_steps < num_timesteps:\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0.0\n",
    "        episode_steps = 0\n",
    "        done = False\n",
    "\n",
    "        last_state_for_ppo = state  # for GAE bootstrapping\n",
    "\n",
    "        while not done:\n",
    "            action_scaled = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action_scaled)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            if algo_name == \"PPO\":\n",
    "                agent.store_reward(reward, done)\n",
    "                if len(agent.buffer['s']) >= ppo_steps_per_update:\n",
    "                    loss = agent.learn(next_state if not done else None)\n",
    "                    if log_wandb and loss is not None:\n",
    "                        wandb.log({\"train/ppo_loss\": loss}, step=total_steps)\n",
    "            else:\n",
    "                agent.store_transition(state, action_scaled, reward, next_state, done)\n",
    "                if len(agent.memory) >= hyperparams['batch_size']:\n",
    "                    loss = agent.learn()\n",
    "                    if log_wandb and loss is not None:\n",
    "                        wandb.log({\"train/offpolicy_loss\": loss}, step=total_steps)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "            total_steps += 1\n",
    "\n",
    "            if total_steps >= num_timesteps:\n",
    "                break\n",
    "\n",
    "        episodes += 1\n",
    "        if log_wandb:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"train/episode_reward\": episode_reward,\n",
    "                    \"train/episode_steps\": episode_steps,\n",
    "                    \"train/episodes\": episodes,\n",
    "                },\n",
    "                step=total_steps,\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            f\"Env: {env_name} | Algo: {algo_name} | \"\n",
    "            f\"Total Steps: {total_steps}/{num_timesteps}, \"\n",
    "            f\"Episode {episodes}, Steps: {episode_steps}, Reward: {episode_reward:.2f}\"\n",
    "        )\n",
    "\n",
    "    env.close()\n",
    "    return agent\n",
    "\n",
    "\n",
    "def test_agent(env_name, agent, algo_name, num_tests=100, seed=123, record_video=False):\n",
    "    video_dir = f\"./videos/{env_name}_{algo_name}\"\n",
    "    if record_video:\n",
    "        env = gym.wrappers.RecordVideo(\n",
    "            make_env(env_name, seed=seed, render_mode=\"rgb_array\"),\n",
    "            video_folder=video_dir,\n",
    "            episode_trigger=lambda i: i == 0,\n",
    "            name_prefix=f\"{algo_name}_test\",\n",
    "        )\n",
    "    else:\n",
    "        env = make_env(env_name, seed=seed, render_mode=None)\n",
    "\n",
    "    rewards = []\n",
    "    for ep in range(num_tests):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0.0\n",
    "        while not done:\n",
    "            action = agent.select_action(state, deterministic=True)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "        rewards.append(ep_reward)\n",
    "\n",
    "    env.close()\n",
    "    avg_r = float(np.mean(rewards))\n",
    "    std_r = float(np.std(rewards))\n",
    "    return avg_r, std_r, rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c9c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 10: Hyperparameters, ENV Config, Agent Factory\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# Total timesteps per environment\n",
    "ENV_TIMESTEPS = {\n",
    "    \"LunarLander-v3\": 300_000,\n",
    "    \"CarRacing-v3\":   100_000,  \n",
    "}\n",
    "\n",
    "ENV_SEEDS = {\n",
    "    \"LunarLander-v3\": 42,\n",
    "    \"CarRacing-v3\":   100,\n",
    "}\n",
    "\n",
    "# Base hyperparams for the 3 algorithms\n",
    "BASE_HYPERPARAMS = {\n",
    "    \"PPO\": {\n",
    "        \"gamma\": 0.99,\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"trajectory_size\": 4096,\n",
    "        \"minibatch_size\": 256,\n",
    "        \"clip_epsilon\": 0.2,\n",
    "        \"ppo_epochs\": 10,\n",
    "        \"entropy_coeff\": 0.01,\n",
    "        \"memory_size\": 0,     # unused\n",
    "        \"batch_size\": 0,       # unused\n",
    "        \"tau\": 0.0,            # unused\n",
    "        \"alpha_start\": 0.0,    # unused\n",
    "    },\n",
    "    \"SAC\": {\n",
    "        \"gamma\": 0.99,\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"lr_critic\": 3e-4,\n",
    "        \"lr_alpha\": 1e-4,\n",
    "        \"memory_size\": 1_000_000,\n",
    "        \"batch_size\": 256,\n",
    "        \"tau\": 0.005,\n",
    "        \"alpha_start\": 0.1,\n",
    "    },\n",
    "    \"TD3\": {\n",
    "        \"gamma\": 0.99,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"lr_critic\": 1e-3,\n",
    "        \"memory_size\": 1_000_000,\n",
    "        \"batch_size\": 256,\n",
    "        \"tau\": 0.005,\n",
    "        \"policy_delay\": 2,\n",
    "        \"policy_noise\": 0.2,\n",
    "        \"noise_clip\": 0.5,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Per-env overrides \n",
    "ENV_ALGO_OVERRIDES = {\n",
    "    \"LunarLander-v3\": {\n",
    "        \"PPO\": {\n",
    "            \"learning_rate\": 3e-4,\n",
    "            \"trajectory_size\": 4096,\n",
    "            \"minibatch_size\": 256,\n",
    "            \"ppo_epochs\": 10,\n",
    "            \"entropy_coeff\": 0.01,\n",
    "        },\n",
    "        \"SAC\": {\n",
    "            \"learning_rate\": 3e-4,\n",
    "            \"lr_critic\": 3e-4,\n",
    "            \"batch_size\": 256,\n",
    "            \"tau\": 0.01,\n",
    "            \"alpha_start\": 0.1,\n",
    "        },\n",
    "        \"TD3\": {\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"lr_critic\": 1e-3,\n",
    "            \"batch_size\": 256,\n",
    "            \"tau\": 0.01,\n",
    "        },\n",
    "    },\n",
    "    \"CarRacing-v3\": {\n",
    "        \"PPO\": {\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"trajectory_size\": 8192,\n",
    "            \"minibatch_size\": 512,\n",
    "            \"ppo_epochs\": 10,\n",
    "            \"entropy_coeff\": 0.0,   \n",
    "        },\n",
    "        \"SAC\": {\n",
    "            \"learning_rate\": 3e-4,\n",
    "            \"lr_critic\": 3e-4,\n",
    "            \"batch_size\": 256,\n",
    "            \"tau\": 0.01,\n",
    "            \"alpha_start\": 0.1,\n",
    "        },\n",
    "        \"TD3\": {\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"lr_critic\": 1e-3,\n",
    "            \"batch_size\": 512,\n",
    "            \"tau\": 0.005,\n",
    "            \"policy_noise\": 0.2,\n",
    "            \"noise_clip\": 0.5,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "ENVIRONMENTS = {\n",
    "    # \"LunarLander-v3\": {},\n",
    "    \"CarRacing-v3\":   {},\n",
    "}\n",
    "\n",
    "SWEEP_VARIATIONS = {\n",
    "    # \"PPO_BASE\": {\"algo\": \"PPO\"},\n",
    "    \"SAC_BASE\": {\"algo\": \"SAC\"},\n",
    "    # \"TD3_BASE\": {\"algo\": \"TD3\"},\n",
    "}\n",
    "\n",
    "def create_agent(env_name, algo_name, config):\n",
    "    # Temporary env to read shapes and action bounds\n",
    "    tmp_env = make_env(env_name, seed=0, render_mode=None)\n",
    "    obs_shape = tmp_env.observation_space.shape\n",
    "    action_dim = tmp_env.action_space.shape[0]\n",
    "    action_bounds = get_action_bounds(tmp_env)\n",
    "    tmp_env.close()\n",
    "\n",
    "    if algo_name == \"PPO\":\n",
    "        return PPOAgent(obs_shape, action_dim, config, action_bounds)\n",
    "    elif algo_name == \"SAC\":\n",
    "        return SACAgent(obs_shape, action_dim, config, action_bounds)\n",
    "    elif algo_name == \"TD3\":\n",
    "        return TD3Agent(obs_shape, action_dim, config, action_bounds)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown algo: {algo_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b8a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 11: W&B + Experiment Runner\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "WANDB_PROJECT = \"CMPS458_SOTA_Continuous_Assignment\"\n",
    "\n",
    "def run_experiment(env_name, algo_name, config, total_timesteps, run_name_suffix=\"\"):\n",
    "    seed = config.get(\"seed\", ENV_SEEDS[env_name])\n",
    "    set_global_seed(seed)\n",
    "\n",
    "    run_name = f\"{env_name}_{algo_name}_{run_name_suffix}\"\n",
    "\n",
    "    wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        config=config,\n",
    "        name=run_name,\n",
    "        reinit=True,\n",
    "    )\n",
    "\n",
    "    # Create agent\n",
    "    agent = create_agent(env_name, algo_name, config)\n",
    "\n",
    "    # Training\n",
    "    print(f\"\\n--- Training {algo_name} on {env_name} for {total_timesteps} steps (seed={seed}) ---\")\n",
    "    trained_agent = train_agent(\n",
    "        env_name,\n",
    "        agent,\n",
    "        algo_name,\n",
    "        config,\n",
    "        num_timesteps=total_timesteps,\n",
    "        seed=seed,\n",
    "        log_wandb=True,\n",
    "    )\n",
    "\n",
    "    # Testing\n",
    "    print(f\"\\n--- Testing {algo_name} on {env_name} (100 episodes) ---\")\n",
    "    avg_r, std_r, rewards = test_agent(\n",
    "        env_name,\n",
    "        trained_agent,\n",
    "        algo_name,\n",
    "        num_tests=100,\n",
    "        seed=seed + 1,\n",
    "        record_video=True, \n",
    "    )\n",
    "\n",
    "    rewards_table = wandb.Table(data=[[r] for r in rewards], columns=[\"episode_reward\"])\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"test/avg_reward\": avg_r,\n",
    "            \"test/std_reward\": std_r,\n",
    "            \"test/rewards_table\": rewards_table,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"RESULTS {run_name}\")\n",
    "    print(f\"  Avg Reward: {avg_r:.2f}\")\n",
    "    print(f\"  Std Reward: {std_r:.2f}\")\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620e6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 12: Main - Run All 3 Algorithms on Both Environments\n",
    "# ============================================================\n",
    "\n",
    "for env_name in ENVIRONMENTS.keys():\n",
    "    total_steps = ENV_TIMESTEPS[env_name]\n",
    "    env_seed = ENV_SEEDS[env_name]\n",
    "\n",
    "    for sweep_name, sweep_conf in SWEEP_VARIATIONS.items():\n",
    "        algo_name = sweep_conf[\"algo\"]\n",
    "\n",
    "        base_hp = BASE_HYPERPARAMS[algo_name].copy()\n",
    "        env_overrides = ENV_ALGO_OVERRIDES[env_name][algo_name]\n",
    "        base_hp.update(env_overrides)\n",
    "\n",
    "        final_config = base_hp.copy()\n",
    "        final_config[\"algo\"] = algo_name\n",
    "        final_config[\"env_name\"] = env_name\n",
    "        final_config[\"seed\"] = env_seed\n",
    "        final_config[\"name\"] = sweep_name\n",
    "\n",
    "        run_experiment(\n",
    "            env_name=env_name,\n",
    "            algo_name=algo_name,\n",
    "            config=final_config,\n",
    "            total_timesteps=total_steps,\n",
    "            run_name_suffix=sweep_name,\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
