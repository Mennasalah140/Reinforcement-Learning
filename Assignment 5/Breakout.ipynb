{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Installations\n",
    "!pip install gymnasium[atari] -q\n",
    "!pip install ale-py -q\n",
    "!pip install autorom[accept-rom-license] -q\n",
    "!pip install wandb -q\n",
    "!pip install huggingface_hub -q\n",
    "!pip install cma -q\n",
    "\n",
    "import os\n",
    "os.makedirs('data/rollouts', exist_ok=True)\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('videos', exist_ok=True)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import cma\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "class Config:\n",
    "    # Environment\n",
    "    ENV_NAME = 'BreakoutNoFrameskip-v4'\n",
    "    IMG_SIZE = 64\n",
    "    NUM_ACTIONS = 4\n",
    "    \n",
    "    # VAE - Fast training to identify the ball\n",
    "    LATENT_SIZE = 32\n",
    "    VAE_EPOCHS = 10       \n",
    "    VAE_BATCH_SIZE = 128   \n",
    "    VAE_LR = 1e-3\n",
    "    \n",
    "    # MDN-RNN (Memory) - Predicting the next frame\n",
    "    HIDDEN_SIZE = 256\n",
    "    NUM_GAUSSIANS = 5\n",
    "    RNN_EPOCHS = 10       \n",
    "    RNN_BATCH_SIZE = 64    \n",
    "    RNN_LR = 1e-3\n",
    "    SEQ_LENGTH = 32        \n",
    "    \n",
    "    # Controller (Motor Skills) - Spend the most time here\n",
    "    POPULATION_SIZE = 32  \n",
    "    NUM_GENERATIONS = 150  \n",
    "    \n",
    "    # Data Collection\n",
    "    NUM_ROLLOUTS = 500     \n",
    "    \n",
    "    # WandB\n",
    "    WANDB_PROJECT = \"cmps458-assignment5\"\n",
    "    USE_WANDB = False\n",
    "\n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Convert Atari RGB frame to 64x64 grayscale\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    resized = cv2.resize(gray, (config.IMG_SIZE, config.IMG_SIZE))\n",
    "    return resized.astype(np.float32) / 255.0\n",
    "\n",
    "def plot_reconstructions(vae, dataset, num_samples=5):\n",
    "    \"\"\"Visualize VAE reconstructions\"\"\"\n",
    "    vae.eval()\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 6))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            original = dataset[i].unsqueeze(0).to(device)\n",
    "            recon, _, _ = vae(original)\n",
    "            \n",
    "            axes[0, i].imshow(original.cpu().squeeze(), cmap='gray')\n",
    "            axes[0, i].axis('off')\n",
    "            axes[0, i].set_title('Original')\n",
    "            \n",
    "            axes[1, i].imshow(recon.cpu().squeeze(), cmap='gray')\n",
    "            axes[1, i].axis('off')\n",
    "            axes[1, i].set_title('Reconstructed')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('vae_reconstructions.png')\n",
    "    plt.show()\n",
    "\n",
    "# VAE Model (AutoEncoder with CNN)\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"Variational AutoEncoder with CNN for frame compression\"\"\"\n",
    "    def __init__(self, latent_size=32, img_channels=1):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # ENCODER: CNN layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, 32, 4, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),           \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),          \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),          \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Latent space\n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_size)\n",
    "        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_size)\n",
    "        \n",
    "        # DECODER: Transposed CNN\n",
    "        self.fc_decode = nn.Linear(latent_size, 256 * 4 * 4)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1), # 4->8\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8->16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),   # 16->32\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, img_channels, 4, stride=2, padding=1), # 32->64\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x).view(x.size(0), -1)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = self.fc_decode(z).view(z.size(0), 256, 4, 4)\n",
    "        return self.decoder(h)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# MDN-RNN Model (World Model with LSTM)\n",
    "class MDRNN(nn.Module):\n",
    "    \"\"\"Mixture Density Network + RNN for world modeling\"\"\"\n",
    "    def __init__(self, latent_size=32, action_size=4, \n",
    "                    hidden_size=256, num_gaussians=5):\n",
    "        super(MDRNN, self).__init__()\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_gaussians = num_gaussians\n",
    "        \n",
    "        # LSTM (RNN component)\n",
    "        self.lstm = nn.LSTM(\n",
    "            latent_size + action_size,\n",
    "            hidden_size,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # MDN outputs\n",
    "        self.mdn_pi = nn.Linear(hidden_size, num_gaussians)\n",
    "        self.mdn_mu = nn.Linear(hidden_size, num_gaussians * latent_size)\n",
    "        self.mdn_sigma = nn.Linear(hidden_size, num_gaussians * latent_size)\n",
    "        \n",
    "        # Auxiliary predictions\n",
    "        self.reward_head = nn.Linear(hidden_size, 1)\n",
    "        self.done_head = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, latent, action, hidden=None):\n",
    "        x = torch.cat([latent, action], dim=-1)\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        # MDN parameters\n",
    "        pi = F.softmax(self.mdn_pi(lstm_out), dim=-1)\n",
    "        mu = self.mdn_mu(lstm_out).view(*lstm_out.shape[:-1], \n",
    "                                            self.num_gaussians, \n",
    "                                            self.latent_size)\n",
    "        sigma = F.elu(self.mdn_sigma(lstm_out)) + 1\n",
    "        sigma = sigma.view(*lstm_out.shape[:-1], \n",
    "                            self.num_gaussians, \n",
    "                            self.latent_size)\n",
    "        \n",
    "        reward = self.reward_head(lstm_out)\n",
    "        done = torch.sigmoid(self.done_head(lstm_out))\n",
    "        \n",
    "        return pi, mu, sigma, reward, done, hidden\n",
    "\n",
    "def mdn_loss(pi, mu, sigma, target):\n",
    "    \"\"\"MDN loss function\"\"\"\n",
    "    target = target.unsqueeze(-2)\n",
    "    normal = torch.distributions.Normal(mu, sigma)\n",
    "    log_prob = normal.log_prob(target).sum(dim=-1)\n",
    "    weighted = log_prob + torch.log(pi + 1e-8)\n",
    "    return -torch.logsumexp(weighted, dim=-1).mean()\n",
    "\n",
    "\n",
    "# Controller Model (Policy Network)\n",
    "class Controller(nn.Module):\n",
    "    \"\"\"Linear controller for action selection\"\"\"\n",
    "    def __init__(self, latent_size=32, hidden_size=256, num_actions=4):\n",
    "        super(Controller, self).__init__()\n",
    "        self.fc = nn.Linear(latent_size + hidden_size, num_actions)\n",
    "    \n",
    "    def forward(self, latent, hidden):\n",
    "        x = torch.cat([latent, hidden], dim=-1)\n",
    "        return torch.softmax(self.fc(x), dim=-1)\n",
    "\n",
    "# Data Collection\n",
    "def collect_random_rollouts(num_rollouts=200):\n",
    "    \"\"\"Generate random gameplay episodes\"\"\"\n",
    "    env = gym.make(config.ENV_NAME)\n",
    "    \n",
    "    print(f\"Collecting {num_rollouts} random rollouts...\")\n",
    "    for i in tqdm(range(num_rollouts)):\n",
    "        observations, actions, rewards, dones = [], [], [], []\n",
    "        \n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done and len(observations) < 1000:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "            observations.append(preprocess_frame(obs))\n",
    "            actions.append(action)\n",
    "            \n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            dones.append(float(done))\n",
    "        \n",
    "        np.savez_compressed(\n",
    "            f'data/rollouts/rollout_{i:04d}.npz',\n",
    "            observations=np.array(observations),\n",
    "            actions=np.array(actions),\n",
    "            rewards=np.array(rewards),\n",
    "            dones=np.array(dones)\n",
    "        )\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "# Run data collection\n",
    "collect_random_rollouts(config.NUM_ROLLOUTS)\n",
    "\n",
    "# VAE Dataset\n",
    "class RolloutDataset(Dataset):\n",
    "    def __init__(self, rollout_dir='data/rollouts'):\n",
    "        self.observations = []\n",
    "        files = [f for f in os.listdir(rollout_dir) if f.endswith('.npz')]\n",
    "        \n",
    "        for f in tqdm(files, desc=\"Loading rollouts\"):\n",
    "            data = np.load(os.path.join(rollout_dir, f))\n",
    "            self.observations.extend(data['observations'])\n",
    "        \n",
    "        self.observations = np.array(self.observations)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.observations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        obs = torch.FloatTensor(self.observations[idx]).unsqueeze(0)\n",
    "        return obs\n",
    "\n",
    "# Create dataset\n",
    "vae_dataset = RolloutDataset()\n",
    "\n",
    "# Train VAE\n",
    "def train_vae(dataset, epochs=5, batch_size=64):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    vae = VAE(latent_size=config.LATENT_SIZE).to(device)\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=config.VAE_LR)\n",
    "    \n",
    "    vae.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(loader, desc=f\"VAE Epoch {epoch+1}/{epochs}\"):\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            recon, mu, logvar = vae(batch)\n",
    "            loss = vae_loss(recon, batch, mu, logvar)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(loader)\n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.2f}\")\n",
    "    \n",
    "    torch.save(vae.state_dict(), 'checkpoints/vae.pth')\n",
    "    return vae\n",
    "\n",
    "# Train VAE\n",
    "vae = train_vae(vae_dataset, epochs=config.VAE_EPOCHS, batch_size=config.VAE_BATCH_SIZE)\n",
    "\n",
    "# Visualize reconstructions\n",
    "plot_reconstructions(vae, vae_dataset)\n",
    "\n",
    "# RNN Dataset\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, rollout_dir, vae, seq_length=32):\n",
    "        self.seq_length = seq_length\n",
    "        self.sequences = []\n",
    "        \n",
    "        files = [f for f in os.listdir(rollout_dir) if f.endswith('.npz')]\n",
    "        \n",
    "        vae.eval()\n",
    "        with torch.no_grad():\n",
    "            for f in tqdm(files, desc=\"Creating sequences\"):\n",
    "                data = np.load(os.path.join(rollout_dir, f))\n",
    "                \n",
    "                obs = torch.FloatTensor(data['observations']).unsqueeze(1).to(device)\n",
    "                mu, _ = vae.encode(obs)\n",
    "                latents = mu.cpu().numpy()\n",
    "                \n",
    "                actions = data['actions']\n",
    "                rewards = data['rewards']\n",
    "                dones = data['dones']\n",
    "                \n",
    "                for i in range(len(latents) - seq_length):\n",
    "                    self.sequences.append({\n",
    "                        'latents': latents[i:i+seq_length],\n",
    "                        'actions': actions[i:i+seq_length],\n",
    "                        'next_latents': latents[i+1:i+seq_length+1],\n",
    "                        'rewards': rewards[i:i+seq_length],\n",
    "                        'dones': dones[i:i+seq_length]\n",
    "                    })\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        \n",
    "        actions_onehot = np.zeros((self.seq_length, config.NUM_ACTIONS))\n",
    "        actions_onehot[np.arange(self.seq_length), seq['actions']] = 1\n",
    "        \n",
    "        return {\n",
    "            'latents': torch.FloatTensor(seq['latents']),\n",
    "            'actions': torch.FloatTensor(actions_onehot),\n",
    "            'next_latents': torch.FloatTensor(seq['next_latents']),\n",
    "            'rewards': torch.FloatTensor(seq['rewards']).unsqueeze(-1),\n",
    "            'dones': torch.FloatTensor(seq['dones']).unsqueeze(-1)\n",
    "        }\n",
    "\n",
    "# Create RNN dataset\n",
    "rnn_dataset = SequenceDataset('data/rollouts', vae, config.SEQ_LENGTH)\n",
    "\n",
    "# Train MDN-RNN\n",
    "def train_mdrnn(dataset, epochs=5, batch_size=16):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    mdrnn = MDRNN(\n",
    "        latent_size=config.LATENT_SIZE,\n",
    "        action_size=config.NUM_ACTIONS,\n",
    "        hidden_size=config.HIDDEN_SIZE,\n",
    "        num_gaussians=config.NUM_GAUSSIANS\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(mdrnn.parameters(), lr=config.RNN_LR)\n",
    "    \n",
    "    mdrnn.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(loader, desc=f\"RNN Epoch {epoch+1}/{epochs}\"):\n",
    "            latents = batch['latents'].to(device)\n",
    "            actions = batch['actions'].to(device)\n",
    "            next_latents = batch['next_latents'].to(device)\n",
    "            rewards = batch['rewards'].to(device)\n",
    "            dones = batch['dones'].to(device)\n",
    "            \n",
    "            pi, mu, sigma, pred_reward, pred_done, _ = mdrnn(latents, actions)\n",
    "            \n",
    "            latent_loss = mdn_loss(pi, mu, sigma, next_latents)\n",
    "            reward_loss = F.mse_loss(pred_reward, rewards)\n",
    "            done_loss = F.binary_cross_entropy(pred_done, dones)\n",
    "            \n",
    "            loss = latent_loss + reward_loss + done_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(mdrnn.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(loader)\n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n",
    "    \n",
    "    torch.save(mdrnn.state_dict(), 'checkpoints/mdrnn.pth')\n",
    "    return mdrnn\n",
    "\n",
    "# Train MDN-RNN\n",
    "mdrnn = train_mdrnn(rnn_dataset, epochs=config.RNN_EPOCHS, batch_size=config.RNN_BATCH_SIZE)\n",
    "\n",
    "# Controller Training with CMA-ES\n",
    "def evaluate_controller(params, vae, mdrnn, num_episodes=3):\n",
    "    \"\"\"Evaluate controller fitness\"\"\"\n",
    "    controller = Controller(\n",
    "        latent_size=config.LATENT_SIZE,\n",
    "        hidden_size=config.HIDDEN_SIZE,\n",
    "        num_actions=config.NUM_ACTIONS\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load parameters\n",
    "    param_dict = {}\n",
    "    offset = 0\n",
    "    for name, param in controller.named_parameters():\n",
    "        size = param.numel()\n",
    "        param_dict[name] = torch.FloatTensor(params[offset:offset+size]).view(param.shape).to(device)\n",
    "        offset += size\n",
    "    controller.load_state_dict(param_dict)\n",
    "    \n",
    "    env = gym.make(config.ENV_NAME)\n",
    "    total_reward = 0\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        hidden = None\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < 1000:\n",
    "            with torch.no_grad():\n",
    "                obs_tensor = torch.FloatTensor(preprocess_frame(obs)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "                latent, _ = vae.encode(obs_tensor)\n",
    "                \n",
    "                if hidden is None:\n",
    "                    hidden_state = torch.zeros(1, config.HIDDEN_SIZE).to(device)\n",
    "                else:\n",
    "                    hidden_state = hidden[0].squeeze(0)\n",
    "                \n",
    "                action_probs = controller(latent, hidden_state)\n",
    "                action = torch.argmax(action_probs).item()\n",
    "            \n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            # Update hidden\n",
    "            if hidden is None:\n",
    "                hidden = (torch.zeros(1, 1, config.HIDDEN_SIZE).to(device),\n",
    "                            torch.zeros(1, 1, config.HIDDEN_SIZE).to(device))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                action_onehot = torch.zeros(1, 1, config.NUM_ACTIONS).to(device)\n",
    "                action_onehot[0, 0, action] = 1\n",
    "                _, _, _, _, _, hidden = mdrnn(latent.unsqueeze(1), action_onehot, hidden)\n",
    "        \n",
    "        total_reward += episode_reward\n",
    "    \n",
    "    env.close()\n",
    "    return total_reward / num_episodes\n",
    "\n",
    "def train_controller(vae, mdrnn):\n",
    "    \"\"\"Train controller using CMA-ES\"\"\"\n",
    "    controller = Controller(config.LATENT_SIZE, config.HIDDEN_SIZE, config.NUM_ACTIONS)\n",
    "    num_params = sum(p.numel() for p in controller.parameters())\n",
    "    \n",
    "    es = cma.CMAEvolutionStrategy(num_params * [0], 0.5, {\n",
    "        'popsize': config.POPULATION_SIZE,\n",
    "        'maxiter': config.NUM_GENERATIONS,\n",
    "        'verbose': -1\n",
    "    })\n",
    "    \n",
    "    best_reward = -float('inf')\n",
    "    best_params = None\n",
    "    \n",
    "    for gen in range(config.NUM_GENERATIONS):\n",
    "        solutions = es.ask()\n",
    "        fitnesses = []\n",
    "        \n",
    "        for sol in solutions:\n",
    "            reward = evaluate_controller(sol, vae, mdrnn)\n",
    "            fitnesses.append(-reward)\n",
    "        \n",
    "        es.tell(solutions, fitnesses)\n",
    "        \n",
    "        current_best = -min(fitnesses)\n",
    "        if current_best > best_reward:\n",
    "            best_reward = current_best\n",
    "            best_params = solutions[np.argmin(fitnesses)]\n",
    "        \n",
    "        print(f\"Generation {gen+1}/{config.NUM_GENERATIONS}: Best={current_best:.2f}, Mean={-np.mean(fitnesses):.2f}\")\n",
    "    \n",
    "    np.save('checkpoints/controller_best.npy', best_params)\n",
    "    return best_params\n",
    "\n",
    "# Train controller\n",
    "best_controller_params = train_controller(vae, mdrnn)\n",
    "\n",
    "# Test and Record Video\n",
    "def test_and_record(vae, mdrnn, controller_params, num_episodes=5):\n",
    "    \"\"\"Test trained agent and record video\"\"\"\n",
    "    \n",
    "    # Load controller\n",
    "    controller = Controller(config.LATENT_SIZE, config.HIDDEN_SIZE, config.NUM_ACTIONS).to(device)\n",
    "    param_dict = {}\n",
    "    offset = 0\n",
    "    for name, param in controller.named_parameters():\n",
    "        size = param.numel()\n",
    "        param_dict[name] = torch.FloatTensor(controller_params[offset:offset+size]).view(param.shape).to(device)\n",
    "        offset += size\n",
    "    controller.load_state_dict(param_dict)\n",
    "    \n",
    "    # Create environment with video recording\n",
    "    env = gym.make(config.ENV_NAME, render_mode='rgb_array')\n",
    "    env = RecordVideo(env, 'videos', episode_trigger=lambda x: True)\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        hidden = None\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < 2000:\n",
    "            with torch.no_grad():\n",
    "                obs_tensor = torch.FloatTensor(preprocess_frame(obs)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "                latent, _ = vae.encode(obs_tensor)\n",
    "                \n",
    "                if hidden is None:\n",
    "                    hidden_state = torch.zeros(1, config.HIDDEN_SIZE).to(device)\n",
    "                else:\n",
    "                    hidden_state = hidden[0].squeeze(0)\n",
    "                \n",
    "                action_probs = controller(latent, hidden_state)\n",
    "                action = torch.argmax(action_probs).item()\n",
    "            \n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if hidden is None:\n",
    "                hidden = (torch.zeros(1, 1, config.HIDDEN_SIZE).to(device),\n",
    "                            torch.zeros(1, 1, config.HIDDEN_SIZE).to(device))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                action_onehot = torch.zeros(1, 1, config.NUM_ACTIONS).to(device)\n",
    "                action_onehot[0, 0, action] = 1\n",
    "                _, _, _, _, _, hidden = mdrnn(latent.unsqueeze(1), action_onehot, hidden)\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "        print(f\"Episode {ep+1}: Reward = {episode_reward:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    print(f\"Average Reward: {np.mean(rewards):.2f} Â± {np.std(rewards):.2f}\")\n",
    "    print(f\"Videos saved in 'videos/' folder\")\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "# Test and record\n",
    "test_rewards = test_and_record(vae, mdrnn, best_controller_params, num_episodes=5)\n",
    "\n",
    "# Save Results and Summary\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(test_rewards)), test_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('World Models Agent Performance on Breakout')\n",
    "plt.savefig('results.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
