{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e13f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 1: Environment Setup\n",
    "# ============================================================\n",
    "import subprocess, sys, os\n",
    "packages = [\"gymnasium[atari,accept-rom-license]\", \"ale-py\", \"numpy==1.26.4\", \"scipy\", \"wandb\", \"stable-baselines3[extra]\", \"shimmy\", \"opencv-python\"]\n",
    "for pkg in packages: subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "# ============================================================\n",
    "# Cell 2: Imports & Global Config\n",
    "# ============================================================\n",
    "import random, numpy as np, torch, torch.nn as nn, torch.optim as optim, cv2\n",
    "import gymnasium as gym, ale_py, wandb\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3 import PPO, DQN\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ENV_NAME = \"SpaceInvadersNoFrameskip-v4\"\n",
    "LATENT_DIM, HIDDEN_DIM, ACTION_DIM = 32, 256, 6\n",
    "CHECKPOINT_DIR, VIDEO_DIR = \"./checkpoints\", \"./videos\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True); os.makedirs(VIDEO_DIR, exist_ok=True)\n",
    "wandb.login(key=\"ec4b83441e852b807a9ee95a4da3288ef3fcf4b3\", relogin=True)\n",
    "\n",
    "# ============================================================\n",
    "# Cell 3: Architecture (VAE, RNN, Controller)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "class SpaceInvadersVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpaceInvadersVAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=1), nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.encoder(torch.zeros(1, 1, 84, 84)).shape[1]\n",
    "        self.fc_mu = nn.Linear(n_flatten, LATENT_DIM)\n",
    "        self.fc_logvar = nn.Linear(n_flatten, LATENT_DIM)\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        return mu + torch.randn_like(mu) * torch.exp(0.5 * logvar)\n",
    "\n",
    "class MDNRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MDNRNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(LATENT_DIM + 1, HIDDEN_DIM, batch_first=True)\n",
    "        self.fc = nn.Linear(HIDDEN_DIM, LATENT_DIM)\n",
    "    def forward(self, z_a, hidden):\n",
    "        out, hidden = self.rnn(z_a, hidden)\n",
    "        return self.fc(out), hidden\n",
    "\n",
    "class Controller(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Controller, self).__init__()\n",
    "        self.fc = nn.Linear(LATENT_DIM + HIDDEN_DIM, ACTION_DIM)\n",
    "    def forward(self, x): return self.fc(x)\n",
    "\n",
    "# ============================================================\n",
    "# Cell 4: Training Loop\n",
    "# ============================================================\n",
    "def train_world_model(episodes=100):\n",
    "    wandb.init(project=\"Assignment5\", name=\"WorldModel_Training\")\n",
    "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "    vae, rnn, ctrl = SpaceInvadersVAE().to(DEVICE), MDNRNN().to(DEVICE), Controller().to(DEVICE)\n",
    "    optimizer = optim.Adam(ctrl.parameters(), lr=1e-3)\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset(); done, ep_reward = False, 0\n",
    "        hidden = (torch.zeros(1, 1, HIDDEN_DIM).to(DEVICE), torch.zeros(1, 1, HIDDEN_DIM).to(DEVICE))\n",
    "        while not done:\n",
    "            obs_gray = cv2.resize(cv2.cvtColor(obs[25:200], cv2.COLOR_RGB2GRAY), (84, 84))\n",
    "            obs_t = torch.from_numpy(obs_gray).float().view(1, 1, 84, 84).to(DEVICE) / 255.0\n",
    "            with torch.no_grad():\n",
    "                mu, logvar = vae.encode(obs_t)\n",
    "                z = vae.reparameterize(mu, logvar)\n",
    "            state_vec = torch.cat([z, hidden[0].view(1, -1)], dim=1)\n",
    "            action = torch.argmax(ctrl(state_vec), dim=1).item()\n",
    "            obs, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc; ep_reward += reward\n",
    "            z_a = torch.cat([z.unsqueeze(1), torch.tensor([[[float(action)]]]).to(DEVICE)], dim=-1)\n",
    "            _, hidden = rnn(z_a, hidden)\n",
    "        \n",
    "        print(f\"World Model | Ep: {ep+1} | Reward: {ep_reward}\")\n",
    "        wandb.log({\"train_reward\": ep_reward, \"episode\": ep})\n",
    "    torch.save(ctrl.state_dict(), \"final_world_model.pt\"); wandb.finish()\n",
    "\n",
    "# ============================================================\n",
    "# Cell 5: Testing & WandB Tables\n",
    "# ============================================================\n",
    "def test_and_table(model_type=\"WM\", episodes=5):\n",
    "    run = wandb.init(project=\"Assignment5\", name=f\"Final_Eval_{model_type}\")\n",
    "    \n",
    "    # TABLE 1: Step-by-step details\n",
    "    step_table = wandb.Table(columns=[\"Episode\", \"Step\", \"Step_Reward\", \"Total_So_Far\"])\n",
    "    \n",
    "    # TABLE 2: Episode summaries and Averages\n",
    "    summary_table = wandb.Table(columns=[\"Episode\", \"Total_Reward\", \"Running_Average\"])\n",
    "    \n",
    "    env = RecordVideo(gym.make(ENV_NAME, render_mode=\"rgb_array\"), f\"{VIDEO_DIR}/eval_{model_type.lower()}\")\n",
    "    \n",
    "    # Load Models\n",
    "    if model_type == \"WM\":\n",
    "        vae, rnn, ctrl = SpaceInvadersVAE().to(DEVICE), MDNRNN().to(DEVICE), Controller().to(DEVICE)\n",
    "        print(ctrl.state_dict())\n",
    "        ctrl.load_state_dict(torch.load(\"final_world_model.pt\")); ctrl.eval()\n",
    "        print(ctrl.state_dict())\n",
    "    else:\n",
    "        model = PPO.load(\"ppo_model\") if model_type == \"PPO\" else DQN.load(\"ddqn_model\")\n",
    "\n",
    "    all_episode_rewards = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset(); done, ep_total_reward, step = False, 0, 0\n",
    "        hidden = (torch.zeros(1, 1, HIDDEN_DIM).to(DEVICE), torch.zeros(1, 1, HIDDEN_DIM).to(DEVICE))\n",
    "        \n",
    "        while not done:\n",
    "            if model_type == \"WM\":\n",
    "                obs_gray = cv2.resize(cv2.cvtColor(obs[25:200], cv2.COLOR_RGB2GRAY), (84, 84))\n",
    "                obs_t = torch.from_numpy(obs_gray).float().view(1, 1, 84, 84).to(DEVICE) / 255.0\n",
    "                with torch.no_grad():\n",
    "                    mu, _ = vae.encode(obs_t)\n",
    "                    state_vec = torch.cat([mu, hidden[0].view(1, -1)], dim=1)\n",
    "                    action = torch.argmax(ctrl(state_vec), dim=1).item()\n",
    "            else:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                \n",
    "            obs, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc; ep_total_reward += reward; step += 1\n",
    "            \n",
    "            # Log to Step Table\n",
    "            step_table.add_data(ep + 1, step, reward, ep_total_reward)\n",
    "            \n",
    "        # Calculate Running Average\n",
    "        all_episode_rewards.append(ep_total_reward)\n",
    "        avg_reward = np.mean(all_episode_rewards)\n",
    "        \n",
    "        # Log to Summary Table\n",
    "        summary_table.add_data(ep + 1, ep_total_reward, avg_reward)\n",
    "        \n",
    "        print(f\"EVAL {model_type} | Episode: {ep+1} | Total Reward: {ep_total_reward} | Avg: {avg_reward:.2f}\")\n",
    "\n",
    "    # Upload both tables to WandB\n",
    "    run.log({\n",
    "        \"detailed_step_analysis\": step_table,\n",
    "        \"episode_performance_summary\": summary_table\n",
    "    })\n",
    "    run.finish()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Cell 6: Bonus Model-Free\n",
    "# ============================================================\n",
    "class RewardPrintingCallback(BaseCallback):\n",
    "    def __init__(self, name): super().__init__(); self.name = name\n",
    "    def _on_step(self):\n",
    "        if \"episode\" in self.locals[\"infos\"][0]:\n",
    "            print(f\"{self.name} | Reward: {self.locals['infos'][0]['episode']['r']}\")\n",
    "        return True\n",
    "\n",
    "def run_baselines(timesteps=30000):\n",
    "    for name, algo in [(\"PPO\", PPO), (\"DDQN\", DQN)]:\n",
    "        wandb.init(project=\"Assignment5\", name=f\"{name}_Training\")\n",
    "        env = Monitor(gym.make(ENV_NAME, render_mode=\"rgb_array\"))\n",
    "        model = algo(\"CnnPolicy\", env, verbose=1).learn(total_timesteps=timesteps, callback=RewardPrintingCallback(name))\n",
    "        model.save(f\"{name.lower()}_model\"); wandb.finish()\n",
    "\n",
    "# ============================================================\n",
    "# Main Entry Point\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    import ale_py\n",
    "    train_world_model(episodes=2000)\n",
    "    # run_baselines(timesteps=500000)\n",
    "    # TESTING PHASE\n",
    "    for m in [\"WM\"]: test_and_table(m)\n",
    "    # for m in [\"WM\", \"PPO\", \"DDQN\"]: test_and_table(m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
