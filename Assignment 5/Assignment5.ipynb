{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc0227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, os\n",
    "\n",
    "# 1. Uninstall everything first to clear the broken \"AutoresetMode\" state\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"gymnasium\", \"ale-py\", \"shimmy\", \"stable-baselines3\"])\n",
    "\n",
    "# 2. Install specific compatible versions\n",
    "packages = [\n",
    "    \"numpy\", \n",
    "    \"gymnasium[atari,accept-rom-license]\", \n",
    "    \"ale-py\", \n",
    "    \"stable-baselines3[extra]\", \n",
    "    \"shimmy\", \n",
    "    \"wandb\", \n",
    "    \"opencv-python\"\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "print(\"--- Setup Complete. RESTART KERNEL NOW ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e13f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Optimized Hyperparameters for >1000 Reward\n",
    "# ============================================================\n",
    "# import subprocess, sys, os\n",
    "\n",
    "import random, numpy as np, torch, torch.nn as nn, torch.optim as optim, cv2, os\n",
    "import gymnasium as gym, ale_py, wandb\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3 import PPO, DQN\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ENV_NAME = \"SpaceInvadersNoFrameskip-v4\"\n",
    "# ENV_NAME = \"ALE/SpaceInvaders-v5\"\n",
    "# OPTIMIZED: Higher dimensions for better feature extraction\n",
    "LATENT_DIM, HIDDEN_DIM, ACTION_DIM = 64, 512, 6 \n",
    "CHECKPOINT_DIR, VIDEO_DIR = \"./checkpoints\", \"./videos\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True); os.makedirs(VIDEO_DIR, exist_ok=True)\n",
    "wandb.login(key=\"ec4b83441e852b807a9ee95a4da3288ef3fcf4b3\", relogin=True)\n",
    "\n",
    "LATENT_DIM, HIDDEN_DIM, ACTION_DIM = 128, 512, 6 \n",
    "LEARNING_RATE = 1e-4 # Slower learning for better convergence\n",
    "\n",
    "# ============================================================\n",
    "# High-Capacity Architecture\n",
    "# ============================================================\n",
    "class SpaceInvadersVAE(nn.Module):\n",
    "    def __init__(self, channels=4):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=1), nn.ReLU(), # Increased filter depth\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.encoder(torch.zeros(1, channels, 84, 84)).shape[1]\n",
    "        self.fc_mu = nn.Linear(n_flatten, LATENT_DIM)\n",
    "        self.fc_logvar = nn.Linear(n_flatten, LATENT_DIM)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "class MDNRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MDNRNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(LATENT_DIM + 1, HIDDEN_DIM, batch_first=True)\n",
    "        self.fc = nn.Linear(HIDDEN_DIM, LATENT_DIM)\n",
    "    def forward(self, z_a, hidden):\n",
    "        out, hidden = self.rnn(z_a, hidden)\n",
    "        return self.fc(out), hidden\n",
    "\n",
    "\n",
    "class Controller(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Wider and Deeper Network with Dropout for Robustness\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(LATENT_DIM + HIDDEN_DIM, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1), # Prevents over-fitting to specific alien patterns\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Linear(256, ACTION_DIM)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "# ============================================================\n",
    "# Advanced Preprocessing (Removing Distractions)\n",
    "# ============================================================\n",
    "def preprocess_pro(obs):\n",
    "    # Crop: removing the top score (0:25) and bottom floor (200:)\n",
    "    # This forces the VAE to focus only on game objects\n",
    "    crop = obs[30:195, :, :] \n",
    "    gray = cv2.cvtColor(crop, cv2.COLOR_RGB2GRAY)\n",
    "    return cv2.resize(gray, (84, 84), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "class FrameStacker:\n",
    "    def __init__(self, size=4):\n",
    "        self.size, self.buffer = size, []\n",
    "    def stack(self, frame):\n",
    "        if not self.buffer: self.buffer = [frame] * self.size\n",
    "        else: self.buffer.pop(0); self.buffer.append(frame)\n",
    "        return torch.from_numpy(np.stack(self.buffer)).float().unsqueeze(0).to(DEVICE) / 255.0\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Training Loop with Reward Clipping & High Episodes\n",
    "# ============================================================\n",
    "def train_high_perf_wm(episodes=3000):\n",
    "    wandb.init(project=\"Assignment5\", name=\"WM_Elite_Training\")\n",
    "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "    vae, rnn, ctrl = SpaceInvadersVAE().to(DEVICE), MDNRNN().to(DEVICE), Controller().to(DEVICE)\n",
    "    optimizer = optim.Adam(ctrl.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset(); stacker = FrameStacker(4); done, ep_reward = False, 0\n",
    "        hidden = (torch.zeros(1, 1, HIDDEN_DIM).to(DEVICE), torch.zeros(1, 1, HIDDEN_DIM).to(DEVICE))\n",
    "        \n",
    "        # Epsilon-Greedy with longer exploration phase\n",
    "        epsilon = max(0.05, 1.0 - (ep / 1500)) \n",
    "\n",
    "        while not done:\n",
    "            obs_t = stacker.stack(preprocess_pro(obs))\n",
    "            with torch.no_grad():\n",
    "                mu, logvar = vae.encode(obs_t)\n",
    "                z = mu # Use Mu for more stable control signals\n",
    "            \n",
    "            state_vec = torch.cat([z, hidden[0].view(1, -1)], dim=1)\n",
    "            \n",
    "            if random.random() < epsilon: action = env.action_space.sample()\n",
    "            else: action = torch.argmax(ctrl(state_vec), dim=1).item()\n",
    "            \n",
    "            obs, reward, term, trunc, _ = env.step(action)\n",
    "            survival_bonus = 0.1 \n",
    "            reward += survival_bonus\n",
    "            \n",
    "            # REWARD SHAPING: Penalize standing still if no enemies hit\n",
    "            if action == 0: reward -= 0.1 \n",
    "            \n",
    "            done = term or trunc; ep_reward += reward\n",
    "            z_a = torch.cat([z.unsqueeze(1), torch.tensor([[[float(action)]]]).to(DEVICE)], dim=-1)\n",
    "            _, hidden = rnn(z_a, hidden)\n",
    "        print(f\"WM Train | Ep: {ep+1} | Reward: {ep_reward}\")\n",
    "        wandb.log({\"reward\": ep_reward, \"eps\": epsilon})\n",
    "        if (ep + 1) % 500 == 0: torch.save(ctrl.state_dict(), f\"elite_wm_{ep}.pt\")\n",
    "    torch.save(ctrl.state_dict(), \"final_world_model.pt\"); wandb.finish()\n",
    "\n",
    "def evaluate_all_stochastic(episodes=10):\n",
    "    for m_type in [\"WM\", \"PPO\", \"DDQN\"]:\n",
    "        print(f\"\\n--- Starting Stochastic Evaluation for {m_type} ---\")\n",
    "        run = wandb.init(project=\"Assignment5\", name=f\"Stochastic_Eval_{m_type}\")\n",
    "        \n",
    "        # Tables for statistical reporting (Mean/StdDev)\n",
    "        summary_tbl = wandb.Table(columns=[\"Ep\", \"Score\", \"Best_So_Far\"])\n",
    "        \n",
    "        # Setup Video Recording\n",
    "        video_dir = f\"{VIDEO_DIR}/stochastic_{m_type.lower()}\"\n",
    "        env = RecordVideo(gym.make(ENV_NAME, render_mode=\"rgb_array\"), video_dir, episode_trigger=lambda x: True)\n",
    "        \n",
    "        # Load Model Logic\n",
    "        if m_type == \"WM\":\n",
    "            vae, rnn, ctrl = SpaceInvadersVAE().to(DEVICE), MDNRNN().to(DEVICE), Controller().to(DEVICE)\n",
    "            ctrl.load_state_dict(torch.load(\"final_world_model.pt\"), strict=False)\n",
    "            ctrl.eval()\n",
    "        else:\n",
    "            try:\n",
    "                model = PPO.load(\"ppo_model\") if m_type == \"PPO\" else DQN.load(\"ddqn_model\")\n",
    "            except:\n",
    "                print(f\"Skipping {m_type}: Model file not found.\")\n",
    "                run.finish(); continue\n",
    "\n",
    "        best_score = -1\n",
    "        scores = []\n",
    "\n",
    "        for ep in range(episodes):\n",
    "            obs, _ = env.reset(); stacker = FrameStacker(4)\n",
    "            done, total_r = False, 0\n",
    "            hidden = (torch.zeros(1, 1, HIDDEN_DIM).to(DEVICE), torch.zeros(1, 1, HIDDEN_DIM).to(DEVICE))\n",
    "            \n",
    "            while not done:\n",
    "                if m_type == \"WM\":\n",
    "                    obs_t = stacker.stack(preprocess_pro(obs))\n",
    "                    with torch.no_grad():\n",
    "                        mu, _ = vae.encode(obs_t)\n",
    "                        logits = ctrl(torch.cat([mu, hidden[0].view(1, -1)], dim=1))\n",
    "                        # STOCHASTIC: Multinomial sampling for variance\n",
    "                        action = torch.multinomial(torch.softmax(logits, dim=1), 1).item()\n",
    "                else:\n",
    "                    # STOCHASTIC: deterministic=False\n",
    "                    action, _ = model.predict(obs, deterministic=False)\n",
    "                \n",
    "                obs, reward, term, trunc, _ = env.step(action)\n",
    "                done = term or trunc; total_r += reward\n",
    "            \n",
    "            scores.append(total_r)\n",
    "            if total_r > best_score: best_score = total_r\n",
    "            summary_tbl.add_data(ep+1, total_r, best_score)\n",
    "            print(f\"{m_type} Ep {ep+1}: {total_r}\")\n",
    "\n",
    "        run.log({\"eval_summary\": summary_tbl, \"mean_reward\": np.mean(scores)})\n",
    "        env.close(); run.finish()\n",
    "        \n",
    "# ============================================================\n",
    "# Main Execution\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # To reach 1000+, you need significantly more episodes \n",
    "    # and the deeper Controller architecture provided above.\n",
    "    train_high_perf_wm(episodes=5000) \n",
    "    evaluate_all_stochastic(episodes=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
