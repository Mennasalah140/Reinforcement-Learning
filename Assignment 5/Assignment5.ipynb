{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc0227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, os\n",
    "\n",
    "# 1. Uninstall everything first to clear the broken \"AutoresetMode\" state\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"gymnasium\", \"ale-py\", \"shimmy\", \"stable-baselines3\"])\n",
    "\n",
    "# 2. Install specific compatible versions\n",
    "packages = [\n",
    "    \"numpy\", \n",
    "    \"gymnasium[atari,accept-rom-license]\", \n",
    "    \"ale-py\", \n",
    "    \"stable-baselines3[extra]\", \n",
    "    \"shimmy\", \n",
    "    \"wandb\", \n",
    "    \"opencv-python\"\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "print(\"--- Setup Complete. RESTART KERNEL NOW ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e13f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Imports & Global Config\n",
    "# ============================================================\n",
    "import random, numpy as np, torch, torch.nn as nn, torch.optim as optim, cv2\n",
    "import gymnasium as gym, ale_py, wandb\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3 import PPO, DQN\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ENV_NAME = \"SpaceInvadersNoFrameskip-v4\"\n",
    "# OPTIMIZED: Higher dimensions for better feature extraction\n",
    "LATENT_DIM, HIDDEN_DIM, ACTION_DIM = 64, 512, 6 \n",
    "CHECKPOINT_DIR, VIDEO_DIR = \"./checkpoints\", \"./videos\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True); os.makedirs(VIDEO_DIR, exist_ok=True)\n",
    "wandb.login(key=\"ec4b83441e852b807a9ee95a4da3288ef3fcf4b3\", relogin=True)\n",
    "\n",
    "# ============================================================\n",
    "# Cell 3: Architecture (V, M, C) with Frame Stacking\n",
    "# ============================================================\n",
    "class SpaceInvadersVAE(nn.Module):\n",
    "    def __init__(self, channels=4): # 4 channels for stacked frames\n",
    "        super(SpaceInvadersVAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, 8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=1), nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.encoder(torch.zeros(1, channels, 84, 84)).shape[1]\n",
    "        self.fc_mu = nn.Linear(n_flatten, LATENT_DIM)\n",
    "        self.fc_logvar = nn.Linear(n_flatten, LATENT_DIM)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        return mu + torch.randn_like(mu) * torch.exp(0.5 * logvar)\n",
    "\n",
    "class MDNRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MDNRNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(LATENT_DIM + 1, HIDDEN_DIM, batch_first=True)\n",
    "        self.fc = nn.Linear(HIDDEN_DIM, LATENT_DIM)\n",
    "    def forward(self, z_a, hidden):\n",
    "        out, hidden = self.rnn(z_a, hidden)\n",
    "        return self.fc(out), hidden\n",
    "\n",
    "class Controller(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Controller, self).__init__()\n",
    "        self.ln = nn.LayerNorm(LATENT_DIM + HIDDEN_DIM)\n",
    "        self.fc = nn.Linear(LATENT_DIM + HIDDEN_DIM, ACTION_DIM)\n",
    "    def forward(self, x): return self.fc(self.ln(x))\n",
    "\n",
    "# ============================================================\n",
    "# Cell 4: Utilities (Preprocessing & Frame Stacking)\n",
    "# ============================================================\n",
    "def preprocess(obs):\n",
    "    gray = cv2.cvtColor(obs[25:200], cv2.COLOR_RGB2GRAY)\n",
    "    return cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "class FrameStacker:\n",
    "    def __init__(self, size=4):\n",
    "        self.size, self.buffer = size, []\n",
    "    def stack(self, frame):\n",
    "        if not self.buffer: self.buffer = [frame] * self.size\n",
    "        else: self.buffer.pop(0); self.buffer.append(frame)\n",
    "        return torch.from_numpy(np.stack(self.buffer)).float().unsqueeze(0).to(DEVICE) / 255.0\n",
    "\n",
    "# ============================================================\n",
    "# Cell 5: Training Loop (World Model)\n",
    "# ============================================================\n",
    "def train_world_model(episodes=200):\n",
    "    wandb.init(project=\"Assignment5\", name=\"WorldModel_Train_Stochastic\")\n",
    "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "    vae, rnn, ctrl = SpaceInvadersVAE().to(DEVICE), MDNRNN().to(DEVICE), Controller().to(DEVICE)\n",
    "    optimizer = optim.Adam(ctrl.parameters(), lr=1e-4)\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset(); stacker = FrameStacker(4); done, ep_reward = False, 0\n",
    "        hidden = (torch.zeros(1, 1, HIDDEN_DIM).to(DEVICE), torch.zeros(1, 1, HIDDEN_DIM).to(DEVICE))\n",
    "        epsilon = max(0.1, 1.0 - (ep / 100)) # Simple exploration decay\n",
    "\n",
    "        while not done:\n",
    "            obs_t = stacker.stack(preprocess(obs))\n",
    "            with torch.no_grad():\n",
    "                mu, logvar = vae.encode(obs_t)\n",
    "                z = vae.reparameterize(mu, logvar)\n",
    "            \n",
    "            state_vec = torch.cat([z, hidden[0].view(1, -1)], dim=1)\n",
    "            \n",
    "            # Training Exploration\n",
    "            if random.random() < epsilon: action = env.action_space.sample()\n",
    "            else: action = torch.argmax(ctrl(state_vec), dim=1).item()\n",
    "            \n",
    "            obs, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc; ep_reward += reward\n",
    "            z_a = torch.cat([z.unsqueeze(1), torch.tensor([[[float(action)]]]).to(DEVICE)], dim=-1)\n",
    "            _, hidden = rnn(z_a, hidden)\n",
    "        \n",
    "        print(f\"WM Train | Ep: {ep+1} | Reward: {ep_reward}\")\n",
    "        wandb.log({\"train_reward\": ep_reward, \"episode\": ep})\n",
    "    \n",
    "    torch.save(ctrl.state_dict(), \"final_world_model.pt\"); wandb.finish()\n",
    "\n",
    "# ============================================================\n",
    "# Cell 6: Evaluation (Non-Deterministic + Best Video)\n",
    "# ============================================================\n",
    "def test_stochastic(model_type=\"WM\", episodes=10):\n",
    "    run = wandb.init(project=\"Assignment5\", name=f\"Stochastic_Eval_{model_type}\")\n",
    "    summ_tbl = wandb.Table(columns=[\"Episode\", \"Total_Reward\", \"Best_Reward_So_Far\"])\n",
    "    \n",
    "    # Record ALL episodes so we can pick the best one later\n",
    "    env = RecordVideo(gym.make(ENV_NAME, render_mode=\"rgb_array\"), f\"{VIDEO_DIR}/eval_{model_type.lower()}\", episode_trigger=lambda x: True)\n",
    "    \n",
    "    vae, rnn, ctrl = SpaceInvadersVAE().to(DEVICE), MDNRNN().to(DEVICE), Controller().to(DEVICE)\n",
    "    if os.path.exists(\"final_world_model.pt\"):\n",
    "        ctrl.load_state_dict(torch.load(\"final_world_model.pt\"), strict=False)\n",
    "    ctrl.eval()\n",
    "\n",
    "    best_reward, best_ep = -1, -1\n",
    "    all_rewards = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset(); stacker = FrameStacker(4); done, ep_r = False, 0\n",
    "        hidden = (torch.zeros(1, 1, HIDDEN_DIM).to(DEVICE), torch.zeros(1, 1, HIDDEN_DIM).to(DEVICE))\n",
    "        \n",
    "        while not done:\n",
    "            obs_t = stacker.stack(preprocess(obs))\n",
    "            with torch.no_grad():\n",
    "                mu, _ = vae.encode(obs_t)\n",
    "                # STOCHASTIC: Sample from softmax instead of argmax\n",
    "                logits = ctrl(torch.cat([mu, hidden[0].view(1, -1)], dim=1))\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                action = torch.multinomial(probs, 1).item()\n",
    "            \n",
    "            obs, reward, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc; ep_r += reward\n",
    "            \n",
    "        all_rewards.append(ep_r)\n",
    "        if ep_r > best_reward: best_reward, best_ep = ep_r, ep\n",
    "        summ_tbl.add_data(ep+1, ep_r, best_reward)\n",
    "        print(f\"STOCHASTIC TEST | Ep: {ep+1} | Reward: {ep_r}\")\n",
    "\n",
    "    print(f\"\\nâœ… BEST REWARD: {best_reward} found in Episode: {best_ep+1}\")\n",
    "    run.log({\"performance_summary\": summ_tbl, \"best_score\": best_reward})\n",
    "    env.close(); run.finish()\n",
    "\n",
    "# ============================================================\n",
    "# Cell 6: Model-Free Baselines (DDQN & PPO Training)\n",
    "# ============================================================\n",
    "def run_baselines(timesteps=500000):\n",
    "    for name, algo in [(\"PPO\", PPO), (\"DDQN\", DQN)]:\n",
    "        print(f\"\\n--- Training {name} ---\")\n",
    "        wandb.init(project=\"Assignment5\", name=f\"{name}_Training\")\n",
    "        \n",
    "        # Standard SB3 Monitor for reward tracking\n",
    "        env = Monitor(gym.make(ENV_NAME, render_mode=\"rgb_array\"))\n",
    "        \n",
    "        # DDQN Fix: Limit buffer size to avoid memory crashes on Kaggle\n",
    "        if name == \"DDQN\":\n",
    "            model = algo(\"CnnPolicy\", env, verbose=1, buffer_size=100000, learning_starts=10000)\n",
    "        else:\n",
    "            model = algo(\"CnnPolicy\", env, verbose=1)\n",
    "            \n",
    "        model.learn(total_timesteps=timesteps)\n",
    "        model.save(f\"{name.lower()}_model\")\n",
    "        wandb.finish()\n",
    "\n",
    "# ============================================================\n",
    "# Cell 7: Stochastic Evaluation for All Models\n",
    "# ============================================================\n",
    "def evaluate_all_stochastic(episodes=10):\n",
    "    for m_type in [\"WM\", \"PPO\", \"DDQN\"]:\n",
    "        print(f\"\\n--- Starting Stochastic Evaluation for {m_type} ---\")\n",
    "        run = wandb.init(project=\"Assignment5\", name=f\"Stochastic_Eval_{m_type}\")\n",
    "        \n",
    "        # Tables for statistical reporting (Mean/StdDev)\n",
    "        summary_tbl = wandb.Table(columns=[\"Ep\", \"Score\", \"Best_So_Far\"])\n",
    "        \n",
    "        # Setup Video Recording\n",
    "        video_dir = f\"{VIDEO_DIR}/stochastic_{m_type.lower()}\"\n",
    "        env = RecordVideo(gym.make(ENV_NAME, render_mode=\"rgb_array\"), video_dir, episode_trigger=lambda x: True)\n",
    "        \n",
    "        # Load Model Logic\n",
    "        if m_type == \"WM\":\n",
    "            vae, rnn, ctrl = SpaceInvadersVAE().to(DEVICE), MDNRNN().to(DEVICE), Controller().to(DEVICE)\n",
    "            ctrl.load_state_dict(torch.load(\"final_world_model.pt\"), strict=False)\n",
    "            ctrl.eval()\n",
    "        else:\n",
    "            try:\n",
    "                model = PPO.load(\"ppo_model\") if m_type == \"PPO\" else DQN.load(\"ddqn_model\")\n",
    "            except:\n",
    "                print(f\"Skipping {m_type}: Model file not found.\")\n",
    "                run.finish(); continue\n",
    "\n",
    "        best_score = -1\n",
    "        scores = []\n",
    "\n",
    "        for ep in range(episodes):\n",
    "            obs, _ = env.reset(); stacker = FrameStacker(4)\n",
    "            done, total_r = False, 0\n",
    "            hidden = (torch.zeros(1, 1, HIDDEN_DIM).to(DEVICE), torch.zeros(1, 1, HIDDEN_DIM).to(DEVICE))\n",
    "            \n",
    "            while not done:\n",
    "                if m_type == \"WM\":\n",
    "                    obs_t = stacker.stack(preprocess(obs))\n",
    "                    with torch.no_grad():\n",
    "                        mu, _ = vae.encode(obs_t)\n",
    "                        logits = ctrl(torch.cat([mu, hidden[0].view(1, -1)], dim=1))\n",
    "                        # STOCHASTIC: Multinomial sampling for variance\n",
    "                        action = torch.multinomial(torch.softmax(logits, dim=1), 1).item()\n",
    "                else:\n",
    "                    # STOCHASTIC: deterministic=False\n",
    "                    action, _ = model.predict(obs, deterministic=False)\n",
    "                \n",
    "                obs, reward, term, trunc, _ = env.step(action)\n",
    "                done = term or trunc; total_r += reward\n",
    "            \n",
    "            scores.append(total_r)\n",
    "            if total_r > best_score: best_score = total_r\n",
    "            summary_tbl.add_data(ep+1, total_r, best_score)\n",
    "            print(f\"{m_type} Ep {ep+1}: {total_r}\")\n",
    "\n",
    "        run.log({\"eval_summary\": summary_tbl, \"mean_reward\": np.mean(scores)})\n",
    "        env.close(); run.finish()\n",
    "\n",
    "# ============================================================\n",
    "# Main Execution\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    import ale_py\n",
    "    # 1. Train MBRL (World Model)\n",
    "    train_world_model(episodes=2000)\n",
    "    \n",
    "    # 2. Train Model-Free (DDQN, PPO) Baselines\n",
    "    # run_baselines(timesteps=500000)\n",
    "    \n",
    "    # 3. Final Evaluation\n",
    "    evaluate_all_stochastic(episodes=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
