{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, os\n",
    "\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"gymnasium\", \"ale-py\", \"shimmy\", \"stable-baselines3\"])\n",
    "\n",
    "packages = [\n",
    "    \"numpy\", \n",
    "    \"gymnasium[atari,accept-rom-license]\", \n",
    "    \"ale-py\", \n",
    "    \"stable-baselines3[extra]\", \n",
    "    \"shimmy\", \n",
    "    \"wandb\", \n",
    "    \"opencv-python\"\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e257ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "import random, numpy as np, torch, torch.nn as nn, torch.optim as optim, cv2, os\n",
    "import gymnasium as gym, ale_py, wandb\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3 import PPO, DQN\n",
    "\n",
    "\n",
    "# Global Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ENV_NAME = \"SpaceInvadersNoFrameskip-v4\"\n",
    "VIDEO_DIR = \"./baseline_videos\"\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "os.makedirs(VIDEO_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "wandb.login(key=\"ec4b83441e852b807a9ee95a4da3288ef3fcf4b3\")\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    # Crop score and floor, grayscale, resize to 84x84\n",
    "    img = frame[30:195, :, :]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    img = cv2.resize(img, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "    return img / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f6f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PPO Architecture (Orthogonal Initialization)\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            self._init_layer(nn.Conv2d(4, 32, 8, stride=4)), nn.ReLU(),\n",
    "            self._init_layer(nn.Conv2d(32, 64, 4, stride=2)), nn.ReLU(),\n",
    "            self._init_layer(nn.Conv2d(64, 64, 3, stride=1)), nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            self._init_layer(nn.Linear(64 * 7 * 7, 512)), nn.ReLU()\n",
    "        )\n",
    "        self.actor = self._init_layer(nn.Linear(512, action_dim), std=0.01)\n",
    "        self.critic = self._init_layer(nn.Linear(512, 1), std=1.0)\n",
    "\n",
    "    def _init_layer(self, layer, std=np.sqrt(2)):\n",
    "        nn.init.orthogonal_(layer.weight, std)\n",
    "        nn.init.constant_(layer.bias, 0)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x): return self.critic(self.network(x))\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        hidden = self.network(x)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = torch.distributions.Categorical(logits=logits)\n",
    "        if action is None: action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd371af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Dueling DDQN Architecture\n",
    "class DuelingDDQN(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, 8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=1), nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.value_stream = nn.Sequential(nn.Linear(64 * 7 * 7, 512), nn.ReLU(), nn.Linear(512, 1))\n",
    "        self.adv_stream = nn.Sequential(nn.Linear(64 * 7 * 7, 512), nn.ReLU(), nn.Linear(512, action_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.conv(x)\n",
    "        v, a = self.value_stream(feat), self.adv_stream(feat)\n",
    "        return v + (a - a.mean(dim=1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ddaf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Testing & Table Logging\n",
    "def run_test(algo, agent, step, episodes=5, test_table=None):\n",
    "    test_env = RecordVideo(gym.make(ENV_NAME, render_mode=\"rgb_array\"), \n",
    "                            f\"{VIDEO_DIR}/{algo}_eval_step_{step}\", episode_trigger=lambda x: True)\n",
    "    total_rewards = []\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        obs, _ = test_env.reset()\n",
    "        stack = deque([preprocess_frame(obs)] * 4, maxlen=4)\n",
    "        ep_r, done = 0, False\n",
    "        while not done:\n",
    "            s_t = torch.from_numpy(np.stack(stack)).float().unsqueeze(0).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                if algo == \"PPO\":\n",
    "                    _, _, _, _ = agent.get_action_and_value(s_t) # Simplified for structure\n",
    "                    logits = agent.actor(agent.network(s_t))\n",
    "                    probs = torch.distributions.Categorical(logits=logits)\n",
    "                    action = probs.sample().item() # Samples based on probability\n",
    "                else:\n",
    "                    epsilon = 0.05  # 5% chance to take a totally random action\n",
    "                    if random.random() < epsilon:\n",
    "                        action = env.action_space.sample() # Random move\n",
    "                    else:\n",
    "                        action = agent(s_t).argmax().item() # Best move\n",
    "            obs, reward, term, trunc, _ = test_env.step(action)\n",
    "            stack.append(preprocess_frame(obs))\n",
    "            ep_r += reward\n",
    "            done = term or trunc\n",
    "        \n",
    "        total_rewards.append(ep_r)\n",
    "        if test_table is not None:\n",
    "            test_table.add_data(step, ep + 1, ep_r)\n",
    "            \n",
    "    test_env.close()\n",
    "    avg_r = np.mean(total_rewards)\n",
    "    return avg_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d92930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Main Training Loop\n",
    "def train(algo=\"PPO\", total_steps=1000000):\n",
    "    wandb.init(project=\"Atari_Elite_Baselines\", name=f\"Scratch_{algo}\", config={\n",
    "        \"algo\": algo, \"total_steps\": total_steps, \"env\": ENV_NAME\n",
    "    })\n",
    "    \n",
    "    # Initialize Table\n",
    "    test_reward_table = wandb.Table(columns=[\"Global_Step\", \"Test_Episode\", \"Reward\"])\n",
    "    \n",
    "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "    action_dim = env.action_space.n\n",
    "    best_avg_reward = -1\n",
    "    \n",
    "    if algo == \"PPO\":\n",
    "        agent = PPOAgent(action_dim).to(DEVICE)\n",
    "        optimizer = optim.Adam(agent.parameters(), lr=2.5e-4, eps=1e-5)\n",
    "    else:\n",
    "        agent = DuelingDDQN(action_dim).to(DEVICE)\n",
    "        optimizer = optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    state_stack = deque([preprocess_frame(obs)] * 4, maxlen=4)\n",
    "    episode_rewards = []\n",
    "    current_ep_reward = 0\n",
    "\n",
    "    for step in range(1, total_steps + 1):\n",
    "        # Action Selection\n",
    "        cur_s = torch.from_numpy(np.stack(state_stack)).float().unsqueeze(0).to(DEVICE)\n",
    "        if algo == \"PPO\":\n",
    "            with torch.no_grad(): action, _, _, _ = agent.get_action_and_value(cur_s)\n",
    "            action = action.item()\n",
    "        else:\n",
    "            eps = max(0.01, 1.0 - (step / 500000))\n",
    "            if random.random() < eps: action = env.action_space.sample()\n",
    "            else: \n",
    "                with torch.no_grad(): action = agent(cur_s).argmax().item()\n",
    "\n",
    "        next_obs, reward, term, trunc, _ = env.step(action)\n",
    "        state_stack.append(preprocess_frame(next_obs))\n",
    "        current_ep_reward += reward\n",
    "        \n",
    "        if term or trunc:\n",
    "            episode_rewards.append(current_ep_reward)\n",
    "            avg_train_r = np.mean(episode_rewards[-10:]) if episode_rewards else 0\n",
    "            print(f\"Step: {step} | Train Reward: {current_ep_reward}\")\n",
    "            wandb.log({\"train/avg_reward_10ep\": avg_train_r, \"train/ep_reward\": current_ep_reward, \"step\": step})\n",
    "            \n",
    "            current_ep_reward = 0\n",
    "            obs, _ = env.reset()\n",
    "            state_stack = deque([preprocess_frame(obs)] * 4, maxlen=4)\n",
    "\n",
    "        # Periodic Testing and Table Logging\n",
    "        if step % 50000 == 0:\n",
    "            avg_test_r = run_test(algo, agent, step, episodes=5, test_table=test_reward_table)\n",
    "            wandb.log({\n",
    "                \"test/avg_reward\": avg_test_r,\n",
    "                \"test/reward_history_table\": test_reward_table,\n",
    "                \"step\": step\n",
    "            })\n",
    "            print(f\"Step: {step} | Test Avg Reward: {avg_test_r}\")\n",
    "            \n",
    "            if avg_test_r > best_avg_reward:\n",
    "                best_avg_reward = avg_test_r\n",
    "                torch.save(agent.state_dict(), f\"{CHECKPOINT_DIR}/{algo}_best.pt\")\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3998d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train(algo=\"PPO\", total_steps=1000000)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
